{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "\n",
    "Now that we have cleaned up all of our data and created a balanced training dataset with SMOTE, we can now create and test different models\n",
    "\n",
    "### Contents:\n",
    "* Data Preparation\n",
    "* Logistic Regression\n",
    "* Random Forest\n",
    "* Naive Bayes\n",
    "* Summary/Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Preparation\n",
    "\n",
    "Importing cleaned training datasets and processing testing dataset to be ready for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing unbalanced data + labels\n",
    "xtrain = pd.read_csv('cleaned_training.csv')\n",
    "ytrain = pd.read_csv('train_labels.csv')\n",
    "\n",
    "# Importing balanced data + labels\n",
    "xtrain_bal = pd.read_csv('balanced_training.csv')\n",
    "ytrain_bal = pd.read_csv('balanced_labels.csv')\n",
    "\n",
    "# Importing test data + labels\n",
    "xtest = pd.read_csv('cleaned_testing.csv')\n",
    "ytest = pd.read_csv('test_labels.csv')\n",
    "\n",
    "# Dropping unused columns from xtrain & ytrain like class and sms\n",
    "xtrain = xtrain.drop(columns = ['class', 'sms'])\n",
    "xtest = xtest.drop(columns = ['class', 'sms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_length</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>...</th>\n",
       "      <th>åômorrow</th>\n",
       "      <th>ìll</th>\n",
       "      <th>ìï</th>\n",
       "      <th>ìïll</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªve</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharry</th>\n",
       "      <th>ûò</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7628 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   char_length  008704050406  0089my  0121  01223585236  01223585334  \\\n",
       "0          158           0.0     0.0   0.0          0.0          0.0   \n",
       "1           24           0.0     0.0   0.0          0.0          0.0   \n",
       "2          148           0.0     0.0   0.0          0.0          0.0   \n",
       "3          110           0.0     0.0   0.0          0.0          0.0   \n",
       "4          143           0.0     0.0   0.0          0.0          0.0   \n",
       "\n",
       "   0125698789   02  0207  02070836089  ...  åômorrow  ìll   ìï  ìïll  ûªm  \\\n",
       "0         0.0  0.0   0.0          0.0  ...       0.0  0.0  0.0   0.0  0.0   \n",
       "1         0.0  0.0   0.0          0.0  ...       0.0  0.0  0.0   0.0  0.0   \n",
       "2         0.0  0.0   0.0          0.0  ...       0.0  0.0  0.0   0.0  0.0   \n",
       "3         0.0  0.0   0.0          0.0  ...       0.0  0.0  0.0   0.0  0.0   \n",
       "4         0.0  0.0   0.0          0.0  ...       0.0  0.0  0.0   0.0  0.0   \n",
       "\n",
       "   ûªt  ûªve   ûï  ûïharry   ûò  \n",
       "0  0.0   0.0  0.0      0.0  0.0  \n",
       "1  0.0   0.0  0.0      0.0  0.0  \n",
       "2  0.0   0.0  0.0      0.0  0.0  \n",
       "3  0.0   0.0  0.0      0.0  0.0  \n",
       "4  0.0   0.0  0.0      0.0  0.0  \n",
       "\n",
       "[5 rows x 7628 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Models\n",
    "\n",
    "I will be testing four different classification models and see how they compare to one another\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- K Nearest Neighbors (KNN)\n",
    "- Naive Bayes\n",
    "\n",
    "And for each, not only will we collect our model's accuracy. But we will also create a confusion matrix and a classification report to get a better understanding of our model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting data for comparison afterwards\n",
    "model_acc = []\n",
    "model_acc_bal = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lg = LogisticRegression(max_iter = 1000)\n",
    "lg_bal = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "lg.fit(xtrain, ytrain['spam'])\n",
    "lg_bal.fit(xtrain_bal, ytrain_bal['spam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Dataset Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619358346927678"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Accuracy\n",
    "lg_pred = lg.predict(xtest)\n",
    "model_acc.append(metrics.accuracy_score(ytest, lg_pred))\n",
    "metrics.accuracy_score(ytest, lg_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1582,   15],\n",
       "       [  55,  187]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "metrics.confusion_matrix(ytest, lg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1597\n",
      "           1       0.93      0.77      0.84       242\n",
      "\n",
      "    accuracy                           0.96      1839\n",
      "   macro avg       0.95      0.88      0.91      1839\n",
      "weighted avg       0.96      0.96      0.96      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report (remember that 0 represents Ham and 1 represents Spam)\n",
    "print(classification_report(ytest,lg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Dataset Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9684611201740077"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_pred_bal = lg_bal.predict(xtest)\n",
    "model_acc_bal.append(metrics.accuracy_score(ytest, lg_pred_bal))\n",
    "metrics.accuracy_score(ytest, lg_pred_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1558,   39],\n",
       "       [  19,  223]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(ytest, lg_pred_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1597\n",
      "           1       0.85      0.92      0.88       242\n",
      "\n",
      "    accuracy                           0.97      1839\n",
      "   macro avg       0.92      0.95      0.93      1839\n",
      "weighted avg       0.97      0.97      0.97      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest,lg_pred_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_bal = RandomForestClassifier()\n",
    "\n",
    "rf.fit(xtrain, ytrain['spam'])\n",
    "rf_bal.fit(xtrain_bal, ytrain_bal['spam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Dataset Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9755301794453507"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pred = rf.predict(xtest)\n",
    "model_acc.append(metrics.accuracy_score(ytest, rf_pred))\n",
    "metrics.accuracy_score(ytest, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1597,    0],\n",
       "       [  45,  197]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(ytest, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      1597\n",
      "           1       1.00      0.81      0.90       242\n",
      "\n",
      "    accuracy                           0.98      1839\n",
      "   macro avg       0.99      0.91      0.94      1839\n",
      "weighted avg       0.98      0.98      0.97      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest,rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Dataset Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9782490483958673"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pred_bal = rf_bal.predict(xtest)\n",
    "model_acc_bal.append(metrics.accuracy_score(ytest, rf_pred_bal))\n",
    "metrics.accuracy_score(ytest, rf_pred_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1595,    2],\n",
       "       [  38,  204]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(ytest, rf_pred_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1597\n",
      "           1       0.99      0.84      0.91       242\n",
      "\n",
      "    accuracy                           0.98      1839\n",
      "   macro avg       0.98      0.92      0.95      1839\n",
      "weighted avg       0.98      0.98      0.98      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest,rf_pred_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb_bal = MultinomialNB()\n",
    "\n",
    "nb.fit(xtrain, ytrain['spam'])\n",
    "nb_bal.fit(xtrain_bal, ytrain_bal['spam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Dataset Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8689505165851006"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_pred = nb.predict(xtest)\n",
    "model_acc.append(metrics.accuracy_score(ytest, nb_pred))\n",
    "metrics.accuracy_score(ytest, nb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1597,    0],\n",
       "       [ 241,    1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(ytest, nb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      1597\n",
      "           1       1.00      0.00      0.01       242\n",
      "\n",
      "    accuracy                           0.87      1839\n",
      "   macro avg       0.93      0.50      0.47      1839\n",
      "weighted avg       0.89      0.87      0.81      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest,nb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Dataset Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9336595976073954"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_pred_bal = nb_bal.predict(xtest)\n",
    "model_acc_bal.append(metrics.accuracy_score(ytest, nb_pred_bal))\n",
    "metrics.accuracy_score(ytest, nb_pred_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1480,  117],\n",
       "       [   5,  237]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(ytest, nb_pred_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96      1597\n",
      "           1       0.67      0.98      0.80       242\n",
      "\n",
      "    accuracy                           0.93      1839\n",
      "   macro avg       0.83      0.95      0.88      1839\n",
      "weighted avg       0.95      0.93      0.94      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest, nb_pred_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary/ Conclusion\n",
    "\n",
    "Let's take a look at our findings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.961936</td>\n",
       "      <td>0.968461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.975530</td>\n",
       "      <td>0.978249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.868951</td>\n",
       "      <td>0.933660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Balanced Accuracy\n",
       "Logistic Regression  0.961936           0.968461\n",
       "Random Forest        0.975530           0.978249\n",
       "Naive Bayes          0.868951           0.933660"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing unbalanced accuracies\n",
    "model_labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes']\n",
    "\n",
    "pd.DataFrame(list(zip(model_acc, model_acc_bal)), index = model_labels, columns = ['Accuracy', 'Balanced Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy \n",
    "\n",
    "It seems that the random forest model performed the best with both the imbalanced and balanced datasets, with around 97% accuracy rate! This was surprising as many online sources mention that Naive Bayes Model are typically better suited for datasets with high dimensions such as this. \n",
    "\n",
    "\n",
    "## Imbalanced vs Balanced\n",
    "\n",
    "Another thing of note is that with all of the models, the models all improved when trained with the balanced dataset. But the degrees of improvement also varied. While the Logistic Regression and Random Forest model saw <1% accuracy increase, the Naive Bayes Model improve 7%! This could be because since the balanced dataset corrected the large imbalance between spam and ham samples, the probabilities calculated in the naive bayes model could have been more accurate/representative of spam and ham differences. As a result of these more representative probabilities, this produced a more accurate model.\n",
    "\n",
    "\n",
    "## Precision vs Recall\n",
    "\n",
    "While Accuracy is important, we also should consider precision and recall. \n",
    "\n",
    "When we look at precision, we can see that all of them had a high precision, with the Naive Bayes trained on the balanced dataset being the highest. But with higher precision also means that our model also has a higher chance of predicting false negatives. This means that these models with high precision have a high chance of predicting spam messages as ham messages.\n",
    "\n",
    "When we look at recall, we can also see that all of the models had high recall, with a three way tie between Random Forest with Imbalanced/Balanced train datasets and Naive bayes with imbalanced dataset. But with higher recall also means that our model also has a higher chance of predicting false positives. This means that these models with high recall have a high chance of predicting ham messages as spam.\n",
    "\n",
    "When it comes to our scenario, spam vs ham sms, we should prioritize higher precision over recall. This is because even though higher precision leads to higher chances of predicting spam messages as ham messages, with high recall models we have a higher change of predicting a ham message as spam. A high recall model may lead us to miss an important ham message since it was labelled spam. But with a high precision model, although we may get some spam messages labelled as ham, fewer of our ham messages will be labeled as spam thus preventing us from lossing valuble information.\n",
    "\n",
    "Great Resource I used for Precision vs Recall: https://towardsdatascience.com/precision-vs-recall-386cf9f89488"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for checking out my SMS Classification Project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
